{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation\n",
    "\n",
    "### Problem 1\n",
    "Calculate the 4th derivative of hyperbolic tangent function using `Jax` autograd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`YOUR SOLUTION`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "Compare analytic and autograd (with any framework) approach for the gradient of:\t\t\n",
    "\t\n",
    "$$\n",
    "f(X) = tr(AXB)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`YOUR SOLUTION`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "\n",
    "In the following code for the gradient descent for linear regression change the manual gradient computation to the PyTorch/jax autograd way. Compare those two approaches in time.\n",
    "\n",
    "In order to do this, set the tolerance rate for the function value $\\varepsilon = 10^{-9}$. Compare the total time required to achieve the specified value of the function for analytical and automatic differentiation. Perform measurements for different values of n from `np.logspace(1,4)`. \n",
    "\n",
    "For each $n$ value carry out at least 3 runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.381, loss = 3.80536386\n",
      "epoch 3: w = 0.938, loss = 1.63591595\n",
      "epoch 5: w = 1.304, loss = 0.70327598\n",
      "epoch 7: w = 1.544, loss = 0.30233650\n",
      "epoch 9: w = 1.701, loss = 0.12997367\n",
      "epoch 11: w = 1.804, loss = 0.05587534\n",
      "epoch 13: w = 1.871, loss = 0.02402066\n",
      "epoch 15: w = 1.916, loss = 0.01032642\n",
      "epoch 17: w = 1.945, loss = 0.00443930\n",
      "epoch 19: w = 1.964, loss = 0.00190844\n",
      "Prediction after training: f(5) = 9.853\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "# Compute every step manually\n",
    "# Problem dimension\n",
    "n = 10\n",
    "\n",
    "# Linear regression\n",
    "# f = w * x \n",
    "\n",
    "# here : f = 2 * x\n",
    "X = np.random.randn(n)\n",
    "Y = 2*X\n",
    "\n",
    "w = 0.0\n",
    "\n",
    "# model output\n",
    "def forward(x):\n",
    "\treturn w * x\n",
    "\n",
    "# loss = MSE\n",
    "def loss(y, y_pred):\n",
    "\treturn ((y_pred - y)**2).mean()\n",
    "\n",
    "# J = MSE = 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N * 2x(w*x - y)\n",
    "def gradient(x, y, y_pred):\n",
    "\treturn np.dot(2*x, y_pred - y).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "\t# predict = forward pass\n",
    "\ty_pred = forward(X)\n",
    "\n",
    "\t# loss\n",
    "\tl = loss(Y, y_pred)\n",
    "\t\n",
    "\t# calculate gradients\n",
    "\tdw = gradient(X, Y, y_pred)\n",
    "\n",
    "\t# update weights\n",
    "\tw -= learning_rate * dw\n",
    "\n",
    "\tif epoch % 2 == 0:\n",
    "\t\tprint(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\t\t\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`YOUR SOLUTION`"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cabbb257f4601b72b329dc5b19245ca917aa0b792d360b345e9f5732308bcb0c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
