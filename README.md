---
title: 🏡 Главная
permalink: /
nav_order: 1
---

# Optimization methods. MSAI 2022

The course is a summary of state-of-the-art results and approaches in solving applied optimization problems. Despite the focus on applications, the course contains the necessary set of theoretical foundations to understand why and how given methods work.

Classes are taken online twice a week for an hour and a half. In the lecture session a brief theoretical introduction to the topic is discussed, in the practical interactive session students solve problems on the topic on their own with Q&A.

## Program

### Week 0

| Градиентный спуск. [🐍 code](https://colab.research.google.com/github/MerkulovDaniil/sber219/blob/main/notebooks/6_01.ipynb){: .btn} | Практика: Одномерный поиск для выбора гиперпараметров модели машинного обучения. Реализация метода градиентного спуска и иследование его численных свойств на задаче выбора оптимальных координат. [🐍 code](https://colab.research.google.com/github/MerkulovDaniil/sber219/blob/main/notebooks/6_1.ipynb){: .btn} |
| Субградиент. Субдифференциал.  [🐍 code](https://colab.research.google.com/github/MerkulovDaniil/sber219/blob/main/notebooks/6_02.ipynb){: .btn} | Практика: Подсчет субградиентов. [🐍 code](https://colab.research.google.com/github/MerkulovDaniil/sber219/blob/main/notebooks/6_2.ipynb){: .btn}
| Метод проекции субградиента. [🐍 code](https://colab.research.google.com/github/MerkulovDaniil/sber219/blob/main/notebooks/6_03.ipynb){: .btn} | Практика: Support Vector Machine как задача оптимизации. Lasso regression. [🐍 code](https://colab.research.google.com/github/MerkulovDaniil/sber219/blob/main/notebooks/6_3.ipynb){: .btn} |

### Week 1

<table>
<thead>
  <tr>
    <th>🦄 Lecture</th>
    <th>🏛 Seminar</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>Brief recap of matrix calculus.</td>
    <td>Examples of matrix and vector derivatives. <br> <a href="https://colab.research.google.com/github/MerkulovDaniil/sber219/blob/main/notebooks/6_3.ipynb" class="btn">🐍 code</a> </td>
  </tr>
  <tr>
    <td>Idea of automatic differentiation.</td>
    <td>Work with automatic differentiation libraries - jax, pytorch, autograd.</td>
  </tr>
</tbody>
</table>

| 🦄 Lecture | 🏛 Seminar |
| Brief recap of matrix calculus. | Examples of matrix and vector derivatives. | 
| Idea of automatic differentiation. | Work with automatic differentiation libraries - jax, pytorch, autograd. |

### Week 2

| 🦄 Lecture | 🏛 Seminar |
| Markowitz portfolio theory. | Financial portfolio optimization based on the actual stock market data. | 
| Introduction to Linear Programing. Idea of simplex method.| Production planning as a linear programming problem in PyOMO. Blending problem. |

### Week 3

| 🦄 Lecture | 🏛 Seminar |
| Introduction to Mixed Integer Programming. | Knapsack problem. |
| Zero order methods and global optimization illustration. | Hyperparameter search for neural network training in Keras with Optuna library.  | 

### Week 4

| 🦄 Lecture | 🏛 Seminar |
|  |  | 
|  |  |

### Week 5

| 🦄 Lecture | 🏛 Seminar |
|  |  | 
|  |  |
### Week 6

| 🦄 Lecture | 🏛 Seminar |
| Stochastic gradient descent method. Batches, epochs, scheduling. Nesterov Momentum and Polyak Momentum. Accelerated Gradient Method. Adaptive Stochastic Methods: Adam, RMSProp, AdaDelta. | The convergence of SGD.  Hyperparameter choice. Convergence study of accelerated methods in learning neural networks. A study of the convergence of adaptive methods in the training of neural networks. | 
|  |  |

### Week 7

| 🦄 Lecture | 🏛 Seminar |
|  |  | 
|  |  |

* [📧 Chat](https://t.me/+kokUwlZ9ClBlYWZi)
* [👨‍💻 Github](https://github.com/MerkulovDaniil/msai22)