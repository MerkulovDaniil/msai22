---
title: ğŸ¡ Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ
permalink: /
nav_order: 1
---

# Optimization methods. MSAI 2022

The course is a summary of state-of-the-art results and approaches in solving applied optimization problems. Despite the focus on applications, the course contains the necessary set of theoretical foundations to understand why and how given methods work.

Classes are taken online twice a week for an hour and a half. In the lecture session a brief theoretical introduction to the topic is discussed, in the practical interactive session students solve problems on the topic on their own with Q&A.

## Program

### Week 0

| Ğ“Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ ÑĞ¿ÑƒÑĞº. [ğŸ code](https://colab.research.google.com/github/MerkulovDaniil/sber219/blob/main/notebooks/6_01.ipynb){: .btn} | ĞŸÑ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°: ĞĞ´Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¿ÑƒÑĞºĞ° Ğ¸ Ğ¸ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞµĞ³Ğ¾ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚. [ğŸ code](https://colab.research.google.com/github/MerkulovDaniil/sber219/blob/main/notebooks/6_1.ipynb){: .btn} |
| Ğ¡ÑƒĞ±Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚. Ğ¡ÑƒĞ±Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ».  [ğŸ code](https://colab.research.google.com/github/MerkulovDaniil/sber219/blob/main/notebooks/6_02.ipynb){: .btn} | ĞŸÑ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°: ĞŸĞ¾Ğ´ÑÑ‡ĞµÑ‚ ÑÑƒĞ±Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². [ğŸ code](https://colab.research.google.com/github/MerkulovDaniil/sber219/blob/main/notebooks/6_2.ipynb){: .btn}
| ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¸ ÑÑƒĞ±Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ°. [ğŸ code](https://colab.research.google.com/github/MerkulovDaniil/sber219/blob/main/notebooks/6_03.ipynb){: .btn} | ĞŸÑ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°: Support Vector Machine ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Lasso regression. [ğŸ code](https://colab.research.google.com/github/MerkulovDaniil/sber219/blob/main/notebooks/6_3.ipynb){: .btn} |

### Week 1

<table>
<thead>
  <tr>
    <th>ğŸ¦„ Lecture</th>
    <th>ğŸ› Seminar</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>Brief recap of matrix calculus.</td>
    <td>Examples of matrix and vector derivatives. <br> <a href="https://colab.research.google.com/github/MerkulovDaniil/sber219/blob/main/notebooks/6_3.ipynb" class="btn">ğŸ code</a> </td>
  </tr>
  <tr>
    <td>Idea of automatic differentiation.</td>
    <td>Work with automatic differentiation libraries - jax, pytorch, autograd.</td>
  </tr>
</tbody>
</table>

| ğŸ¦„ Lecture | ğŸ› Seminar |
| Brief recap of matrix calculus. | Examples of matrix and vector derivatives. | 
| Idea of automatic differentiation. | Work with automatic differentiation libraries - jax, pytorch, autograd. |

### Week 2

| ğŸ¦„ Lecture | ğŸ› Seminar |
| Markowitz portfolio theory. | Financial portfolio optimization based on the actual stock market data. | 
| Introduction to Linear Programing. Idea of simplex method.| Production planning as a linear programming problem in PyOMO. Blending problem. |

### Week 3

| ğŸ¦„ Lecture | ğŸ› Seminar |
| Introduction to Mixed Integer Programming. | Knapsack problem. |
| Zero order methods and global optimization illustration. | Hyperparameter search for neural network training in Keras with Optuna library.  | 

### Week 4

| ğŸ¦„ Lecture | ğŸ› Seminar |
|  |  | 
|  |  |

### Week 5

| ğŸ¦„ Lecture | ğŸ› Seminar |
|  |  | 
|  |  |
### Week 6

| ğŸ¦„ Lecture | ğŸ› Seminar |
| Stochastic gradient descent method. Batches, epochs, scheduling. Nesterov Momentum and Polyak Momentum. Accelerated Gradient Method. Adaptive Stochastic Methods: Adam, RMSProp, AdaDelta. | The convergence of SGD.  Hyperparameter choice. Convergence study of accelerated methods in learning neural networks. A study of the convergence of adaptive methods in the training of neural networks. | 
|  |  |

### Week 7

| ğŸ¦„ Lecture | ğŸ› Seminar |
|  |  | 
|  |  |

* [ğŸ“§ Chat](https://t.me/+kokUwlZ9ClBlYWZi)
* [ğŸ‘¨â€ğŸ’» Github](https://github.com/MerkulovDaniil/msai22)