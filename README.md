---
title: ğŸ¡ Home
permalink: /
nav_order: 1
---

# Optimization methods. MSAI 2022

The course is a summary of state-of-the-art results and approaches in solving applied optimization problems. Despite the focus on applications, the course contains the necessary set of theoretical foundations to understand why and how given methods work.

Classes are taken online twice a week for an hour and a half. In the lecture session a brief theoretical introduction to the topic is discussed, in the practical interactive session students solve problems on the topic on their own with Q&A.

## Program

### Week 1

<table>
<thead>
  <tr>
    <th>ğŸ¦„ Lecture</th>
    <th>ğŸ› Seminar</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>Brief recap of matrix calculus. <br> <a href="/presentations/1_1.pdf" class="btn">ğŸ“„ presentation </a> <a href="/notes/1_1.pdf" class="btn">ğŸ“ notes</a> <a href="https://youtu.be/Ia0UfweJk5o" class="btn">ğŸ“¼ video</a></td>
    <td>Examples of matrix and vector derivatives. <br> <a href="https://colab.research.google.com/github/MerkulovDaniil/msai22/blob/main/notebooks/1_1.ipynb" class="btn">ğŸ code</a> </td>
  </tr>
  <tr>
    <td>Idea of automatic differentiation. <br> <a href="/presentations/1_2.pdf" class="btn">ğŸ“„ presentation</a> <a href="/notes/1_2.pdf" class="btn">ğŸ“ notes</a>  <a href="https://colab.research.google.com/github/MerkulovDaniil/msai22/blob/main/notebooks/1_2_Autograd.ipynb"  class="btn">ğŸ code</a> <a href="#" class="btn">ğŸ“¼ video</a></td>
    <td>Work with automatic differentiation libraries - jax, pytorch, autograd. <br> <a href="https://colab.research.google.com/github/MerkulovDaniil/msai22/blob/main/notebooks/1_2.ipynb"  class="btn">ğŸ code</a></td>
  </tr>
</tbody>
</table>

* [ğŸ“§ Chat](https://t.me/+kokUwlZ9ClBlYWZi)
* [ğŸ‘¨â€ğŸ’» Github](https://github.com/MerkulovDaniil/msai22)
